{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 5: Dual Camera Setups\n",
    "\n",
    "Now that we know how to model a camera, and understand the challenges of going from 2D to 3D, let's see how to use two cameras to get a 3D point. By adding a second camera, we add a second viewpoint, which can help us triangulate the 3D point. This is the basis for stereo vision, which is a very common technique in computer vision.\n",
    "\n",
    "## Transformation Matrix\n",
    "\n",
    "Last session we looked at how the Rodrigues formula can be used to describe a rotation. We combined this with a translation to get a full rigid body transformation. I really recommend seeing 3Blue1Brown's linear algebra series, especially the episode on [Three-dimensional linear transformations](https://www.youtube.com/watch?v=rHLEWRxRGiM&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=6&ab_channel=3Blue1Brown) to get a better understanding of how this works.\n",
    "\n",
    "Our implementation of the transformation matrix can be seen inside our [module](../oaf_vision_3d/transformation_matrix.py). \n",
    "\n",
    "## Triangulation\n",
    "\n",
    "### Intersecting lines\n",
    "\n",
    "To triangulate we try to calculate the intersection between two lines in 3d. In our case the first line comes from the main camera:\n",
    "\n",
    "$$ L_0 = P_0 + \\vec{v}_0t $$\n",
    "\n",
    "Where $L_0$ represents all points along the line. $P_0$ is the camera origo, which for us will be located at origo. $\\vec{v}_0$ is the undistorted normalized camera pixel $\\vec{v}_0 = [x''_0, y''_0, 1]$ and $t$ is any real number used to define the points along the line. Since $P_0$ is at origo, we can simplify this to:\n",
    "\n",
    "$$ L_0 = \\vec{v}_0t $$\n",
    "\n",
    "The second line is defined using the undistorted normalized projector pixels in the same way:\n",
    "\n",
    "$$ L_1 = P_1 + R\\vec{u}_1s$$\n",
    "\n",
    "Where $L_1$ represents all points along the second line. $P_1$ and $R$ are the transformation between main camera and the second camera. $\\vec{u}_1$ is the undistorted normalized pixels for the second camera, $\\vec{u}_1 = [x''_1, y''_1, 1]$. $s$ is any real number used to define the points along the line. We can simplify this to:\n",
    "\n",
    "$$ L_1 = P_1 + \\vec{v}_1s$$\n",
    "\n",
    "Where $\\vec{v}_1$ is the rotated vector $\\vec{v}_1 = R\\vec{u}_1$.\n",
    "\n",
    "### Closest distance\n",
    "\n",
    "The problem with two lines in 3D is that the very rarely actually collide. Thus, we solve for the closest distance between the lines. We can start to define a point along $P$ for any $t$. The closest distance from this point to $L_1$ is where we are normal to the line. This also holds for points on $L_1$ to $L_0$. Hence, there must be a point $P(t)$ on $L_0$ and a point $Q(s)$ on $L_1$:\n",
    "\n",
    "$$ \\begin{align*} P &= P_0 + t\\vec{v}_0 \\\\ Q &= P_1 + s\\vec{v}_1 \\end{align*} $$\n",
    "\n",
    "for $t$ and $s$ such that:\n",
    "\n",
    "$$PQ = Q - P$$\n",
    "\n",
    "is normal to both $\\vec{v}_0$ and $\\vec{v}_1$\n",
    "\n",
    "  $$ \\begin{align*} \\vec{v}_0 \\cdot PQ &= 0 \\\\ \\vec{v}_1 \\cdot PQ &= 0 \\end{align*} $$\n",
    "\n",
    "We can multiply this out to:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(\\vec{v}_0 \\cdot \\vec{v}_0)t - (\\vec{v}_0 \\cdot \\vec{v}_1)s &= \\vec{v}_0 \\cdot (P_1 - P_0) \\\\\n",
    "(\\vec{v}_0 \\cdot \\vec{v}_1)t - (\\vec{v}_1 \\cdot \\vec{v}_1)s &= \\vec{v}_1 \\cdot (P_1 - P_0)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and get $Ax=B$ system:\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{cc} a & -b \\\\ b & -c \\end{array}\\right]\n",
    "\\left[\\begin{array}{cc} t \\\\ s \\end{array}\\right] =\n",
    "\\left[\\begin{array}{cc} d \\\\ e \\end{array}\\right]\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a &= \\vec{v}_0 \\cdot \\vec{v}_0 \\\\\n",
    "b &= \\vec{v}_0 \\cdot \\vec{v}_1 \\\\\n",
    "c &= \\vec{v}_1 \\cdot \\vec{v}_1 \\\\\n",
    "d &= \\vec{v}_0 \\cdot (P_1 - P_0) \\\\\n",
    "e &= \\vec{v}_0 \\cdot (P_1 - P_0)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Which gives us the solution:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "t &= \\frac{be-cd}{b^2-ac} \\\\\n",
    "s &= \\frac{ae-bd}{b^2-ac}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since we use the main camera as our reference, we want to keep the point in the main camera's coordinate system. Hence, we can calculate the 3D point as:\n",
    "\n",
    "$$ P = P_0 + t\\vec{v}_0 $$\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Setup\n",
    "\n",
    "Let us first create a dataset we can work with, we can utilize the previous tools we have made to create a dataset between two cameras. We start by loading a [`Lens Model`](../oaf_vision_3d/lens_model.py) for both cameras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from oaf_vision_3d.lens_model import CameraMatrix, DistortionCoefficients, LensModel\n",
    "\n",
    "\n",
    "lens_model_0 = LensModel(\n",
    "    camera_matrix=CameraMatrix(\n",
    "        fx=2500.0,\n",
    "        fy=2500.0,\n",
    "        cx=1250.0,\n",
    "        cy=1000.0,\n",
    "    ),\n",
    "    distortion_coefficients=DistortionCoefficients(\n",
    "        k1=0.3,\n",
    "        k2=-0.1,\n",
    "        p1=-0.02,\n",
    "    ),\n",
    ")\n",
    "lens_model_1 = deepcopy(lens_model_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a list of points in 3D space and use [project_points](../oaf_vision_3d/project_points.py) to project these points into the main camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from oaf_vision_3d.project_points import project_points\n",
    "\n",
    "\n",
    "points = np.stack(\n",
    "    [\n",
    "        *np.meshgrid(np.linspace(-20, 20, 10), np.linspace(-20, 20, 10)),\n",
    "        100 * np.ones((10, 10)),\n",
    "    ],\n",
    "    axis=-1,\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "pixels_0 = project_points(\n",
    "    points=points.reshape(-1, 3),\n",
    "    lens_model=lens_model_0,\n",
    ").reshape(10, 10, 2)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(pixels_0[..., 0], pixels_0[..., 1], \"x-\")\n",
    "plt.plot(pixels_0[..., 0].T, pixels_0[..., 1].T, \"-\")\n",
    "plt.axis(\"equal\")\n",
    "plt.xlim(0, 2499)\n",
    "plt.ylim(1999, 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then decide upon whatever transform we want between the first and second camera. We can use this to project the points into the second camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from oaf_vision_3d.transformation_matrix import TransformationMatrix\n",
    "\n",
    "\n",
    "rvec = np.array([-0.002, -0.15, 0.001], dtype=np.float32)\n",
    "tvec = np.array([15.0, 0.2, 2.1], dtype=np.float32)\n",
    "transformation_matrix = TransformationMatrix.from_rvec_and_tvec(rvec=rvec, tvec=tvec)\n",
    "\n",
    "pixels_1 = project_points(\n",
    "    points=points.reshape(-1, 3),\n",
    "    lens_model=lens_model_1,\n",
    "    transformation_matrix=transformation_matrix.inverse(),\n",
    ").reshape(10, 10, 2)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(pixels_1[..., 0], pixels_1[..., 1], \"x-\")\n",
    "plt.plot(pixels_1[..., 0].T, pixels_1[..., 1].T, \"-\")\n",
    "plt.axis(\"equal\")\n",
    "plt.xlim(0, 2499)\n",
    "plt.ylim(1999, 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "Now that we have the data to triangulate we first need to prepare the data, what we need is:\n",
    "\n",
    "- The undistorted normalized pixels for the main camera\n",
    "- The undistorted normalized pixels for the second camera\n",
    "- The transformation between the two cameras\n",
    "\n",
    "We can calculate the undistorted normalized pixels of the cameras by using their respective lens models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "undistorted_normalized_pixels_0 = lens_model_0.undistort_pixels(\n",
    "    lens_model_0.normalize_pixels(pixels=pixels_0)\n",
    ")\n",
    "undistorted_normalized_pixels_1 = lens_model_1.undistort_pixels(\n",
    "    lens_model_1.normalize_pixels(pixels=pixels_1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangulate\n",
    "\n",
    "We can than triangulate using the math discussed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nptyping import Float32, NDArray, Shape\n",
    "\n",
    "\n",
    "def triangulate_points(\n",
    "    undistorted_normalized_pixels_0: NDArray[Shape[\"H, W, 2\"], Float32],\n",
    "    undistorted_normalized_pixels_1: NDArray[Shape[\"H, W, 2\"], Float32],\n",
    "    transformation_matrix: TransformationMatrix,\n",
    ") -> NDArray[Shape[\"H, W, 3\"], Float32]: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "trianguated_points = triangulate_points(\n",
    "    undistorted_normalized_pixels_0=undistorted_normalized_pixels_0,\n",
    "    undistorted_normalized_pixels_1=undistorted_normalized_pixels_1,\n",
    "    transformation_matrix=transformation_matrix,\n",
    ")\n",
    "trianguated_points = (\n",
    "    np.zeros_like(points) if trianguated_points is None else trianguated_points\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "We can test the implementation by comparing the triangulated points with the original 3D points, feel free to go back and try different transformations between the cameras and/or different 3D points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "valid = np.allclose(points, trianguated_points, atol=1e-5)\n",
    "\n",
    "print(f\"Triangualtion {'succeeded' if valid else 'failed'}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "We plot the 3D points and the triangulated points to see how well we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(points[..., 0], points[..., 1], points[..., 2], c=\"r\", marker=\"o\")\n",
    "ax.scatter(\n",
    "    trianguated_points[..., 0],\n",
    "    trianguated_points[..., 1],\n",
    "    trianguated_points[..., 2],\n",
    "    c=\"b\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "ax.axis(\"equal\")\n",
    "ax.invert_yaxis()\n",
    "ax.invert_zaxis()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
