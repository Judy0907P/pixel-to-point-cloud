{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 7: Stereo Matching Fundamentals Continued\n",
    "\n",
    "In the [previous workshop](./06_stereo_matching_fundamentals.ipynb), we looked at stereo matching fundamentals. More precisely, we looked at the disparity map computation using the block matching algorithm. In this workshop, we will continue with stereo matching fundamentals and how we can do this without using disparity, that assumes a linear shift in $x$ direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epipolar Geometry\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nptyping import Float32, NDArray, Shape\n",
    "import numpy as np\n",
    "from scipy.ndimage import map_coordinates\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.signal.windows import gaussian\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "from oaf_vision_3d._test_data_paths import TestDataPaths\n",
    "from oaf_vision_3d.convolve2d import convolution_2d_nan\n",
    "from oaf_vision_3d.lens_model import CameraMatrix, DistortionCoefficients, LensModel\n",
    "from oaf_vision_3d.point_cloud_visualization import open3d_visualize_point_cloud\n",
    "from oaf_vision_3d.poly_2_subvalue_fit import find_subvalue_poly_2\n",
    "from oaf_vision_3d.project_points import project_points\n",
    "from oaf_vision_3d.transformation_matrix import TransformationMatrix\n",
    "from oaf_vision_3d._stereo_data_reader import StereoData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load up a lens model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens_model = LensModel(\n",
    "    camera_matrix=CameraMatrix(fx=2500.0, fy=2500.0, cx=1250.0, cy=1000.0),\n",
    "    distortion_coefficients=DistortionCoefficients(k1=0.3, k2=-0.1, p1=-0.02),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a transformation matrix between the two cameras, for now explained with the same lens model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_matrix = TransformationMatrix(\n",
    "    rotation=Rotation.from_rotvec(\n",
    "        np.array([0.0, -11.31, 0.0], dtype=np.float32), degrees=True\n",
    "    ),\n",
    "    translation=np.array([100.0, 0.0, 0.0], dtype=np.float32),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now pick a random pixel, let's say (1250, 1000) in the main camera, and a depth of 500 mm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel = np.array([1250.0, 1000.0], dtype=np.float32)\n",
    "depth = 500.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the 3d position of this point in the main camera, if that pixel was at that depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "undistorted_pixel = lens_model.undistort_pixels(\n",
    "    normalized_pixels=lens_model.normalize_pixels(pixels=pixel[None, None, :])\n",
    ")\n",
    "xyz = np.pad(undistorted_pixel, ((0, 0), (0, 0), (0, 1)), constant_values=1.0) * depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can project this point onto the second camera, by transforming by the inverse of the transformation matrix, since we go from the main camera to the second camera. We can use the [`project_points`](../oaf_vision_3d/project_points.py) function for this and return the pixel position in the second camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_point = project_points(\n",
    "    points=xyz[0],\n",
    "    lens_model=lens_model,\n",
    "    transformation_matrix=transformation_matrix.inverse(),\n",
    ")[0]\n",
    "\n",
    "print(f\"Projected point, X: {projected_point[0]:.1f}, Y: {projected_point[1]:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out the projected pixel hit in the center of the second camera, this is not a suprise since I designed it this way. With a baseline $t_x = 100$ mm and depth of $500$ mm, the selected angle $\\theta_y = \\arctan(\\frac{t_x}{d})$. \n",
    "\n",
    "Let's do this for a range of depths and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_min = 350.0\n",
    "depth_max = 650.0\n",
    "num_samples = 10\n",
    "\n",
    "undistorted_pixel = lens_model.undistort_pixels(\n",
    "    normalized_pixels=lens_model.normalize_pixels(pixels=pixel[None, None, :])\n",
    ")\n",
    "camera_vector = np.pad(undistorted_pixel, ((0, 0), (0, 0), (0, 1)), constant_values=1.0)\n",
    "\n",
    "depths = np.linspace(depth_min, depth_max, num_samples)\n",
    "projected_points = []\n",
    "for depth in depths:\n",
    "    xyz = camera_vector * depth\n",
    "    projected_points.append(\n",
    "        project_points(\n",
    "            points=xyz[0],\n",
    "            lens_model=lens_model,\n",
    "            transformation_matrix=transformation_matrix.inverse(),\n",
    "        )[0]\n",
    "    )\n",
    "projected_points = np.array(projected_points, dtype=np.float32)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(projected_points[:, 0], projected_points[:, 1], \".-\")\n",
    "plt.xlim(0, 2500)\n",
    "plt.ylim(2000, 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the the point makes up a line, this line is distorted since we use the `project_points` function. We can do this for ta grid around the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = np.stack(\n",
    "    np.meshgrid(np.linspace(250, 2250, 6), np.linspace(200, 1800, 5)), axis=-1\n",
    ").astype(np.float32)\n",
    "undistorted_normalized_pixels = lens_model.undistort_pixels(\n",
    "    normalized_pixels=lens_model.normalize_pixels(pixels=pixels)\n",
    ")\n",
    "camera_vectors = np.pad(\n",
    "    undistorted_normalized_pixels, ((0, 0), (0, 0), (0, 1)), constant_values=1.0\n",
    ")\n",
    "\n",
    "depths = np.linspace(depth_min, depth_max, num_samples)\n",
    "projected_points = []\n",
    "for depth in depths:\n",
    "    xyz = camera_vectors * depth\n",
    "    projected_points.append(\n",
    "        project_points(\n",
    "            points=xyz.reshape(-1, 3),\n",
    "            lens_model=lens_model,\n",
    "            transformation_matrix=transformation_matrix.inverse(),\n",
    "        ).reshape(pixels.shape)\n",
    "    )\n",
    "projected_points = np.array(projected_points, dtype=np.float32)\n",
    "\n",
    "\n",
    "projected_points_reshaped = projected_points.reshape(\n",
    "    -1, pixels.shape[0] * pixels.shape[1], 2\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(projected_points_reshaped[..., 0], projected_points_reshaped[..., 1], \".-\")\n",
    "plt.xlim(0, 2500)\n",
    "plt.ylim(2000, 0)\n",
    "plt.axis(\"image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see how these pixels in the main camera project as lines in the second camera. These lines are called epipolar lines. The epipolar lines are lines in the second camera that correspond to a point in the main camera. This is the basis of stereo matching, we can search for the corresponding point in the second camera along the epipolar line. This is a much easier problem than searching the entire image for the corresponding point. It also work regardless of the orientation of the cameras, as long as we know the transformation between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essential Matrix\n",
    "\n",
    "In todays session we looked at the essential matrix, which is a matrix that describes the transformation between two cameras. It is defined as:\n",
    "\n",
    "$$\n",
    "E = [\\vec{t}]_x R\n",
    "$$\n",
    "\n",
    "Where $[\\vec{t}]_x$ is the skew symmetric matrix of the translation vector $t$ and $R$ is the rotation matrix. The essential matrix is used to calculate the epipolar line, the essential matrix solves:\n",
    "\n",
    "$$\n",
    "\\vec{v}_0^T E \\vec{v}_1 = 0\n",
    "$$\n",
    "\n",
    "Where $\\vec{v}_0$ and $\\vec{v}_1$ are the normalized undistorted pixel coordinates in the two cameras. This gives if we say:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} a & b & c \\end{bmatrix} = \\vec{v}_0^T E\n",
    "$$\n",
    "\n",
    "Then the epipolar line in the second camera is:\n",
    "\n",
    "$$\n",
    "a x''_1 + b y''_1 + c = 0\n",
    "$$\n",
    "\n",
    "Such that we have the line equation:\n",
    "\n",
    "$$\n",
    "y''_1 = -\\frac{a}{b} x''_1 - \\frac{c}{b}\n",
    "$$\n",
    "\n",
    "This mean that for any pixel in the first camera, we can caluclate every pixel we could see in the second camera with simple matrix multiplication. \n",
    "\n",
    "Let's calculate the essential matrix for the example above:\n",
    "\n",
    "::: {admonition} Skew Symmetric Matrix\n",
    ":class: toggle, tip\n",
    "$$\n",
    "[\\vec{v}]_x = \\begin{bmatrix} 0 & -v_z & v_y \\\\ v_z & 0 & -v_x \\\\ -v_y & v_x & 0 \\end{bmatrix}\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skew_symmetric_matrix(\n",
    "    vector: NDArray[Shape[\"3\"], Float32],\n",
    ") -> NDArray[Shape[\"3, 3\"], Float32]:\n",
    "    return np.array(\n",
    "        [\n",
    "            [0.0, -vector[2], vector[1]],\n",
    "            [vector[2], 0.0, -vector[0]],\n",
    "            [-vector[1], vector[0], 0.0],\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "\n",
    "\n",
    "essential_matrix = (\n",
    "    skew_symmetric_matrix(transformation_matrix.translation)\n",
    "    @ transformation_matrix.rotation.as_matrix()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use this to calculate the slope and intercept of the epipolar line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = (camera_vectors @ essential_matrix).reshape(-1, 3)\n",
    "\n",
    "slope = -abc[..., 0] / abc[..., 1]\n",
    "intercept = -abc[..., 2] / abc[..., 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can distort and denormalize to plot on top of the projected points to show that they match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undistorted_normalized_projector_pixels_reshaped = lens_model.undistort_pixels(\n",
    "    normalized_pixels=lens_model.normalize_pixels(pixels=projected_points_reshaped)\n",
    ")\n",
    "\n",
    "x_min = undistorted_normalized_projector_pixels_reshaped[..., 0].min(axis=0)\n",
    "x_max = undistorted_normalized_projector_pixels_reshaped[..., 0].max(axis=0)\n",
    "\n",
    "new_pixels = []\n",
    "for _slope, _intercept, _x_min, _x_max in zip(slope, intercept, x_min, x_max):\n",
    "    x = np.linspace(_x_min, _x_max, 10)\n",
    "    y = _slope * x + _intercept\n",
    "    _new_pixels = np.stack([x, y], axis=-1)\n",
    "    new_pixels.append(\n",
    "        lens_model.denormalize_pixels(\n",
    "            pixels=lens_model.distort_pixels(normalized_pixels=_new_pixels[None, ...])\n",
    "        )[0]\n",
    "    )\n",
    "new_pixels = np.array(new_pixels)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(projected_points_reshaped[..., 0], projected_points_reshaped[..., 1], \".\")\n",
    "plt.plot(new_pixels[..., 0].T, new_pixels[..., 1].T, \"k-\")\n",
    "plt.xlim(0, 2500)\n",
    "plt.ylim(2000, 0)\n",
    "plt.axis(\"image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plane Sweep Algorithm\n",
    "\n",
    "Now we have seen how we can move along a line in the image regardless of the orientation of the cameras. Even though we will not use the epipolar geometry, in form of the essential matrix or the fundamental matrix. We can still use the concept of projecting an image that we worked on in [workshop 3](./03_image_distortion_and_undistortion.ipynb) and [workshop 4](./04_3d_2d_projections_and_pnp.ipynb). To do this we can use a plane sweeping algorithm, let's start by loading the same dataset as in the [previous workshop](./06_stereo_matching_fundamentals.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "data_dir = TestDataPaths.stereo_data_0_dir\n",
    "stereo_data = StereoData.from_path(data_dir)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 12))\n",
    "ax[0].imshow(stereo_data.image_0, cmap=\"gray\", vmin=0, vmax=1)\n",
    "ax[0].set_title(\"Image left\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(stereo_data.image_1, cmap=\"gray\", vmin=0, vmax=1)\n",
    "ax[1].set_title(\"Image right\")\n",
    "ax[1].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start be defining a given distance, for now I will use 140 mm. We can then create a plane in the coordinate system of the main camera, and project this plane onto the second camera. We can then recreate time image by interpolating the image values from the second camera. Lets start by defining the camera vector of the main camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = np.indices(stereo_data.image_0.shape[:2], dtype=np.float32)[::-1].transpose(\n",
    "    (1, 2, 0)\n",
    ")\n",
    "undistorted_normalized_pixels = stereo_data.lens_model_0.undistort_pixels(\n",
    "    normalized_pixels=stereo_data.lens_model_0.normalize_pixels(pixels=pixels)\n",
    ")\n",
    "camera_vectors = np.pad(\n",
    "    undistorted_normalized_pixels, ((0, 0), (0, 0), (0, 1)), constant_values=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we project all those pooint to the second camera and map the coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeoject_image_at_depth(\n",
    "    image: NDArray[Shape[\"H, W, ...\"], Float32],\n",
    "    camera_vectors: NDArray[Shape[\"H, W, 3\"], Float32],\n",
    "    depth: float,\n",
    "    lens_model: LensModel,\n",
    "    transformation_matrix: TransformationMatrix,\n",
    ") -> NDArray[Shape[\"H, W, ...\"], Float32]:\n",
    "    xyz = camera_vectors * depth\n",
    "\n",
    "    projected_points = project_points(\n",
    "        points=xyz.reshape(-1, 3),\n",
    "        lens_model=lens_model,\n",
    "        transformation_matrix=transformation_matrix.inverse(),\n",
    "    ).reshape(*camera_vectors.shape[:2], 2)\n",
    "\n",
    "    return np.stack(\n",
    "        [\n",
    "            map_coordinates(\n",
    "                input=_image,\n",
    "                coordinates=[projected_points[..., 1], projected_points[..., 0]],\n",
    "                order=1,\n",
    "                mode=\"constant\",\n",
    "                cval=np.nan,\n",
    "            )\n",
    "            for _image in image.transpose(2, 0, 1)\n",
    "        ],\n",
    "        axis=-1,\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "\n",
    "\n",
    "depth = 140.0\n",
    "\n",
    "new_image = repeoject_image_at_depth(\n",
    "    image=stereo_data.image_1,\n",
    "    camera_vectors=camera_vectors,\n",
    "    depth=depth,\n",
    "    lens_model=stereo_data.lens_model_1,\n",
    "    transformation_matrix=stereo_data.transformation_matrix,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(new_image, cmap=\"gray\", vmin=0, vmax=1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Reprojected image at depth {depth:.1f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now subtract the difference between the projected image and the main camera image, I'm doing sum of absolute differences here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_diff = np.abs(stereo_data.image_0 - new_image).sum(axis=-1)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image_diff, vmin=-0.1, vmax=0.1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Image difference\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this is similar to what we did with disparity and we might reason that we can do the same algorithm for stereo matching, just using depth instead of disparity. This also allows us to get depth directly, so the trianguilation is built into the algorithm. Let repeat this process for a range of depths, 130 to 150 mm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plane_sweeping(\n",
    "    image_0: NDArray[Shape[\"H, W\"], Float32],\n",
    "    lens_model_0: LensModel,\n",
    "    image_1: NDArray[Shape[\"H, W\"], Float32],\n",
    "    lens_model_1: LensModel,\n",
    "    transformation_matrix: TransformationMatrix,\n",
    "    depth_range: NDArray[Shape[\"2\"], Float32],\n",
    "    step_size: float,\n",
    "    block_size: int,\n",
    "    subpixel_fit: bool,\n",
    ") -> NDArray[Shape[\"H, W, 3\"], Float32]:\n",
    "    pixels = np.indices(image_0.shape[:2], dtype=np.float32)[::-1].transpose((1, 2, 0))\n",
    "    undistorted_normalized_pixels = lens_model_0.undistort_pixels(\n",
    "        normalized_pixels=lens_model_0.normalize_pixels(pixels=pixels)\n",
    "    )\n",
    "    camera_vectors_0 = np.pad(\n",
    "        undistorted_normalized_pixels, ((0, 0), (0, 0), (0, 1)), constant_values=1.0\n",
    "    )\n",
    "\n",
    "    depths = np.arange(\n",
    "        start=depth_range[0],\n",
    "        stop=depth_range[1] + step_size,\n",
    "        step=step_size,\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "    error = []\n",
    "    for depth in depths:\n",
    "        shifted_image_1 = repeoject_image_at_depth(\n",
    "            image=image_1,\n",
    "            camera_vectors=camera_vectors_0,\n",
    "            depth=depth,\n",
    "            lens_model=lens_model_1,\n",
    "            transformation_matrix=transformation_matrix,\n",
    "        )\n",
    "        single_pixel_error = np.abs(image_0 - shifted_image_1).sum(axis=-1)\n",
    "\n",
    "        convoluted_error = convolve2d(\n",
    "            convolve2d(\n",
    "                single_pixel_error, np.ones((1, block_size)) / block_size, mode=\"same\"\n",
    "            ),\n",
    "            np.ones((block_size, 1)) / block_size,\n",
    "            mode=\"same\",\n",
    "        )\n",
    "        error.append(convoluted_error)\n",
    "    error_array = np.array(error, dtype=np.float32)\n",
    "\n",
    "    if subpixel_fit:\n",
    "        output_value = find_subvalue_poly_2(values=depths, function_value=error_array)\n",
    "    else:\n",
    "        output_value = depths[np.argmin(error_array, axis=0)].astype(np.float32)\n",
    "\n",
    "    output_value[output_value >= depths.max()] = np.nan\n",
    "    output_value[output_value <= depths.min()] = np.nan\n",
    "\n",
    "    return camera_vectors_0 * output_value[..., None]\n",
    "\n",
    "\n",
    "xyz_single_plane_sweep = plane_sweeping(\n",
    "    image_0=stereo_data.image_0,\n",
    "    lens_model_0=stereo_data.lens_model_0,\n",
    "    image_1=stereo_data.image_1,\n",
    "    lens_model_1=stereo_data.lens_model_1,\n",
    "    transformation_matrix=stereo_data.transformation_matrix,\n",
    "    depth_range=np.array([130.0, 150.0], dtype=np.float32),\n",
    "    step_size=1.5,\n",
    "    block_size=29,\n",
    "    subpixel_fit=True,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(xyz_single_plane_sweep[..., 2])\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Depth map from single plane sweeping\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "open3d_visualize_point_cloud(xyz=xyz_single_plane_sweep, rgb=stereo_data.image_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Multiple Views\n",
    "\n",
    "Now that we have an approach to match any camera to another camera at any position we are no limited to 1 camera. We can add as many cameras as we want, as long as we know the transformation between them. We will now load 2 dataset, where we add another image to the left of the left image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_data_0 = StereoData.from_path(TestDataPaths.stereo_data_0_dir)\n",
    "stereo_data_1 = StereoData.from_path(TestDataPaths.stereo_data_1_dir)\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(12, 15))\n",
    "ax[0].imshow(stereo_data_1.image_1, cmap=\"gray\", vmin=0, vmax=1)\n",
    "ax[0].set_title(\"Image left\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(stereo_data_0.image_0, cmap=\"gray\", vmin=0, vmax=1)\n",
    "ax[1].set_title(\"Image main\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[2].imshow(stereo_data_0.image_1, cmap=\"gray\", vmin=0, vmax=1)\n",
    "ax[2].set_title(\"Image right\")\n",
    "ax[2].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any distance we could now reproject both the left and right image to the center image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "pixels = np.indices(stereo_data_0.image_0.shape[:2], dtype=np.float32)[::-1].transpose(\n",
    "    (1, 2, 0)\n",
    ")\n",
    "undistorted_normalized_pixels = stereo_data_0.lens_model_0.undistort_pixels(\n",
    "    normalized_pixels=stereo_data_0.lens_model_0.normalize_pixels(pixels=pixels)\n",
    ")\n",
    "camera_vectors_0 = np.pad(\n",
    "    undistorted_normalized_pixels, ((0, 0), (0, 0), (0, 1)), constant_values=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 140.0\n",
    "\n",
    "shifted_image_0 = repeoject_image_at_depth(\n",
    "    image=stereo_data_0.image_1,\n",
    "    camera_vectors=camera_vectors_0,\n",
    "    depth=depth,\n",
    "    lens_model=stereo_data_0.lens_model_1,\n",
    "    transformation_matrix=stereo_data_0.transformation_matrix,\n",
    ")\n",
    "shifted_image_1 = repeoject_image_at_depth(\n",
    "    image=stereo_data_1.image_1,\n",
    "    camera_vectors=camera_vectors_0,\n",
    "    depth=depth,\n",
    "    lens_model=stereo_data_1.lens_model_1,\n",
    "    transformation_matrix=stereo_data_1.transformation_matrix,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(12, 15))\n",
    "ax[0].imshow(shifted_image_1, cmap=\"gray\", vmin=0, vmax=1)\n",
    "ax[0].set_title(\"Image left (shifted)\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(stereo_data_0.image_0, cmap=\"gray\", vmin=0, vmax=1)\n",
    "ax[1].set_title(\"Image main\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[2].imshow(shifted_image_0, cmap=\"gray\", vmin=0, vmax=1)\n",
    "ax[2].set_title(\"Image right (shifted)\")\n",
    "ax[2].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can show the absolute difference between the center image and the reprojected images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_0 = np.abs(stereo_data_0.image_0 - shifted_image_0).sum(axis=-1)\n",
    "diff_1 = np.abs(stereo_data_0.image_0 - shifted_image_1).sum(axis=-1)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 15))\n",
    "ax[0].imshow(diff_1, vmin=0, vmax=0.2)\n",
    "ax[0].set_title(\"Difference left\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(diff_0, vmin=0, vmax=0.2)\n",
    "ax[1].set_title(\"Difference right\")\n",
    "ax[1].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also draw som reference lines compared to the main camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(12, 15))\n",
    "ax[0].imshow(shifted_image_1, cmap=\"gray\", vmin=0, vmax=1)\n",
    "ax[0].plot([0, shifted_image_0.shape[1]], [1000, 1000], \"r-\")\n",
    "ax[0].plot([1280, 1280], [0, shifted_image_0.shape[0]], \"r-\")\n",
    "ax[0].set_title(\"Image left (shifted)\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(stereo_data_0.image_0, cmap=\"gray\", vmin=0, vmax=1)\n",
    "ax[1].plot([0, shifted_image_0.shape[1]], [1000, 1000], \"r-\")\n",
    "ax[1].plot([1280, 1280], [0, shifted_image_0.shape[0]], \"r-\")\n",
    "ax[1].set_title(\"Image main\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[2].imshow(shifted_image_0, cmap=\"gray\", vmin=0, vmax=1)\n",
    "ax[2].plot([0, shifted_image_0.shape[1]], [1000, 1000], \"r-\")\n",
    "ax[2].plot([1280, 1280], [0, shifted_image_0.shape[0]], \"r-\")\n",
    "ax[2].set_title(\"Image right (shifted)\")\n",
    "ax[2].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "You task is now to make a plane sweeping function that no longer takes in a set of image but a main image and any number of cameras. The function should return the depth of the main image. You can use the code below as a starting point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_camera_plane_sweeping(\n",
    "    image: NDArray[Shape[\"H, W, ...\"], Float32],\n",
    "    lens_model: LensModel,\n",
    "    secondary_images: list[NDArray[Shape[\"H, W, ...\"], Float32]],\n",
    "    secondary_lens_models: list[LensModel],\n",
    "    secondary_transformation_matrices: list[TransformationMatrix],\n",
    "    depth_range: NDArray[Shape[\"2\"], Float32],\n",
    "    step_size: float,\n",
    "    block_size: int,\n",
    "    subpixel_fit: bool,\n",
    ") -> NDArray[Shape[\"H, W, 3\"], Float32]: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz_multiple_plane_sweep_ = multi_camera_plane_sweeping(\n",
    "    image=stereo_data_0.image_0,\n",
    "    lens_model=stereo_data_0.lens_model_0,\n",
    "    secondary_images=[stereo_data_0.image_1, stereo_data_1.image_1],\n",
    "    secondary_lens_models=[stereo_data_0.lens_model_1, stereo_data_1.lens_model_1],\n",
    "    secondary_transformation_matrices=[\n",
    "        stereo_data_0.transformation_matrix,\n",
    "        stereo_data_1.transformation_matrix,\n",
    "    ],\n",
    "    depth_range=np.array([135.0, 142.0], dtype=np.float32),\n",
    "    step_size=0.5,\n",
    "    block_size=29,\n",
    "    subpixel_fit=True,\n",
    ")\n",
    "\n",
    "xyz_multiple_plane_sweep = (\n",
    "    xyz_single_plane_sweep\n",
    "    if xyz_multiple_plane_sweep_ is None\n",
    "    else xyz_multiple_plane_sweep_\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(xyz_multiple_plane_sweep[..., 2])\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Depth map from multiple plane sweep\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now used 2 secondary cameras to calculate the depth of the main camera. Regardless of that we still see the limitiation of the stereo matching algorithm, that it is hard to get depth in areas where there is no texture. Let's visualize it with Open3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open3d_visualize_point_cloud(xyz=xyz_multiple_plane_sweep, rgb=stereo_data_0.image_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "To improve the result we can add some simple filtering, for example to reduce the amount of outliers. My idea was to apply a relativly big `ones` kernel and then zap points that are a certain distance away from the filtered depth. Since what we are looking at is smooth and flat, this should work. But remember that it most likely will not work for more complex scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 25\n",
    "limit = 0.5\n",
    "\n",
    "xyz_filtered = xyz_multiple_plane_sweep.copy()\n",
    "\n",
    "z_convolved = convolution_2d_nan(\n",
    "    xyz_filtered[..., 2], np.ones(kernel_size, dtype=np.float32)\n",
    ")\n",
    "invalid = np.abs(xyz_filtered[..., 2] - z_convolved) > limit\n",
    "xyz_filtered[invalid] = np.nan\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(xyz_filtered[..., 2], interpolation=\"none\")\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Depth map (filtered)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open3d_visualize_point_cloud(xyz=xyz_filtered, rgb=stereo_data_0.image_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add a gaussian blur to the resulting point cloud. This should reduce the noise in the point cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz_filtered_smoothed = xyz_filtered.copy()\n",
    "kernel = gaussian(M=7, std=1.5)\n",
    "kernel /= kernel.sum()\n",
    "\n",
    "z_filtered_smoothed = convolution_2d_nan(image=xyz_filtered[..., 2], kernel=kernel)\n",
    "xyz_filtered_smoothed *= z_filtered_smoothed[..., None] / xyz_filtered[..., 2:3]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(xyz_filtered_smoothed[..., 2], interpolation=\"none\")\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Depth map (filtered and smoothed)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open3d_visualize_point_cloud(xyz=xyz_filtered_smoothed, rgb=stereo_data_0.image_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
