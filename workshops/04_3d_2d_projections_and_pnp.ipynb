{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 4: 3D-2D Projections and PnP\n",
    "\n",
    "When we looked at undistorting an image in the previous [session](03_image_distortion_and_undistortion.ipynb) we saw how we could technically \"reproject\" a an undistorted image from a distorted image. This is a very important concept in computer vision, and in this session we will look at how we can project a 3D point to a 2D point. We actually forecasted this in the previous session, and I gave a small hint how this is done. Let revisit the same example. Assume you have a 3D coordinate:\n",
    "\n",
    "$$\\left[\\begin{array}{cc} X \\\\ Y \\\\ Z \\end{array}\\right]$$\n",
    "\n",
    "How can we figure out where this point would be seen in the image? We now actually have a straight forward approach:\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{cc} X \\\\ Y \\\\ Z \\end{array}\\right] \\xrightarrow[Z]{\\mathrm{divide}}\n",
    "\\left[\\begin{array}{cc} \\frac{X}{Z} \\\\ \\frac{Y}{Z} \\\\ 1\\end{array}\\right] \\xrightarrow[kc]{\\mathrm{distort}}\n",
    "\\left[\\begin{array}{cc} x' \\\\ y' \\\\ 1\\end{array}\\right] \\xrightarrow[K]{\\mathrm{denormalize}}\n",
    "\\left[\\begin{array}{cc} x \\\\ y\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "This is the process of projecting a 3D point to a 2D point. The first step is to divide by the depth, which gives us the normalized image coordinates. The second step is to distort the normalized image coordinates. The third step is to denormalize the distorted image coordinates. The resulting image coordinated are where the 3D point would be seen in the image. There are some thing to be aware of:\n",
    "\n",
    "- There is no guarantee that the point will be visible in the image, as it might be outside the image boundaries.\n",
    "- If the point is behind the camera, the point will be projected to the image plane, but it will be projected to the opposite side of the image plane. This is why we need to check the depth of the point before projecting it to the image plane. If the depth is negative, the point is behind the camera, since the camera is defined to be positioned at the origin with the optical axis pointing in the $z$-direction.\n",
    "- If the $z$-coordinate is 0, the division will result in a division by zero. Hence, the projection is not defined at this point. This can be seen as the point being in the plane of the image itself, and at this point the image sensor is \"infinitely\" small.\n",
    "\n",
    "## Simple Projection\n",
    "\n",
    "Let us start by loading a camera calibration and a set of 3D points. For the 3D points I will define a grid in the $xy$-plane, where the $z$-coordinate is 0. This is how we usually define 2D objects like checkerboards etc. in 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from oaf_vision_3d.lens_model import CameraMatrix, DistortionCoefficients, LensModel\n",
    "\n",
    "\n",
    "lens_model = LensModel(\n",
    "    camera_matrix=CameraMatrix(\n",
    "        fx=2500.0,\n",
    "        fy=2500.0,\n",
    "        cx=1250.0,\n",
    "        cy=1000.0,\n",
    "    ),\n",
    "    distortion_coefficients=DistortionCoefficients(\n",
    "        k1=0.3,\n",
    "        k2=-0.1,\n",
    "        p1=-0.02,\n",
    "    ),\n",
    ")\n",
    "\n",
    "points = np.stack(\n",
    "    [\n",
    "        *np.meshgrid(np.linspace(-20, 20, 10), np.linspace(-20, 20, 10)),\n",
    "        np.zeros((10, 10)),\n",
    "    ],\n",
    "    axis=-1,\n",
    "    dtype=np.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose to make a grid that is 10x10, and the grid where $x$ and $y$ are in the range $[-20, 20]$, equally spaced with 10 points in each direction. I think of this as being in mm, since this is the unit I usually work in, but this is unitless.\n",
    "\n",
    "Let's see how this grid look in 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(points[..., 0], points[..., 1], points[..., 2], c=\"r\", marker=\"o\")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "ax.axis(\"equal\")\n",
    "ax.invert_yaxis()\n",
    "ax.invert_zaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first implement a simple function that follows the above equation to project a 3D point to a 2D point. The steps are:\n",
    "\n",
    "1. Divide $X$ and $Y$ by $Z$. This is now our normalized undistorted image coordinates, they are undistorted since we are in 3D space.\n",
    "2. Distort the normalized undistorted image coordinates.\n",
    "3. Denormalize the distorted image coordinates to get the final image coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nptyping import Float32, NDArray, Shape\n",
    "\n",
    "\n",
    "def project_points_simple(\n",
    "    points: NDArray[Shape[\"*, 3\"], Float32],\n",
    "    lens_model: LensModel,\n",
    ") -> NDArray[Shape[\"*, 2\"], Float32]:\n",
    "    undistorted_normalized_pixels = points[None, :, :2] / points[None, :, 2:]\n",
    "    normalized_pixels = lens_model.distort_pixels(\n",
    "        normalized_pixels=undistorted_normalized_pixels\n",
    "    )\n",
    "    return lens_model.denormalize_pixels(pixels=normalized_pixels)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know that the projection is undefined if $Z=0$, so for this inital projection I will set the $Z$-coordinate to 100. Let's first see how the grid looks in 3D, we also add the point $(0, 0, 0)$ to show where the camera is located:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "points_shifted = points + np.array([0, 0, 100], dtype=np.float32)[None, None, :]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(\n",
    "    points_shifted[..., 0],\n",
    "    points_shifted[..., 1],\n",
    "    points_shifted[..., 2],\n",
    "    c=\"r\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "ax.scatter(0, 0, 0, c=\"b\", marker=\"x\")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "ax.axis(\"equal\")\n",
    "ax.invert_yaxis()\n",
    "ax.invert_zaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our simple projection function to project the 3D points to 2D points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "projector_points = project_points_simple(\n",
    "    points=points_shifted.reshape(-1, 3), lens_model=lens_model\n",
    ").reshape(10, 10, 2)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(projector_points[..., 0], projector_points[..., 1], \"x-\")\n",
    "plt.plot(projector_points[..., 0].T, projector_points[..., 1].T, \"-\")\n",
    "plt.axis(\"equal\")\n",
    "plt.xlim(0, 2499)\n",
    "plt.ylim(1999, 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Projection\n",
    "\n",
    "It might be misleading to call it simple and full projection, but in the projection algorithm we usually include a transform of the points. This comes in the form of a rotation and translation, that we apply to the points before projecting. Above I added 100 to all the $Z$-coordinates, which is the same as sending in an translation of $[0, 0, 100]^T$. It is benefitial to use the [Rodrigues rotation vector](https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula) to represent the rotation, since this is a compact representation of the rotation. The rotation vector give the axis of rotation, and the the lenght of the vector gives the angle of rotation. A big benefit with the rotation vector is that:\n",
    "\n",
    "1. It is a compact representation of the rotation using only 3 numbers.\n",
    "2. The rotation vector is differentiable, which is important for optimization algorithms.\n",
    "\n",
    "We will not implement the Rodrigues rotation vector to rotation matrix, but rather use the OpenCV function `cv2.Rodrigues` to convert the rotation vector to a rotation matrix.\n",
    "\n",
    "We can now implement the projection function with rotation and translation using the step-by-step approach:\n",
    "\n",
    "1. Rotate the points using the rotation matrix, which we can directly get from the rotation vector using `cv2.Rodrigues`.\n",
    "2. Translate the points using the translation vector.\n",
    "3. Project the points to the image plane using the projection function we implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def project_points(\n",
    "    points: NDArray[Shape[\"*, 3\"], Float32],\n",
    "    rvec: NDArray[Shape[\"3\"], Float32],\n",
    "    tvec: NDArray[Shape[\"3\"], Float32],\n",
    "    lens_model: LensModel,\n",
    ") -> NDArray[Shape[\"*, 2\"], Float32]:\n",
    "    rotation_matrix, _ = cv2.Rodrigues(rvec)\n",
    "    transformed_points = points @ rotation_matrix.T + tvec[None, :]\n",
    "\n",
    "    return project_points_simple(points=transformed_points, lens_model=lens_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use an interactive plot to show where the 2D points end up as a function of the 6 parameters in the projection function, $r_x$, $r_y$, $r_z$, $t_x$, $t_y$ and $t_z$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "\n",
    "def plot_with_sliders(\n",
    "    rx: float, ry: float, rz: float, tx: float, ty: float, tz: float\n",
    ") -> None:\n",
    "    rvec = np.array([rx, ry, rz], dtype=np.float32)\n",
    "    tvec = np.array([tx, ty, tz], dtype=np.float32)\n",
    "\n",
    "    pixels = project_points(\n",
    "        points=points.reshape(-1, 3), rvec=rvec, tvec=tvec, lens_model=lens_model\n",
    "    ).reshape(10, 10, 2)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(pixels[..., 0], pixels[..., 1], \"x-\")\n",
    "    plt.plot(pixels[..., 0].T, pixels[..., 1].T, \"-\")\n",
    "    plt.axis(\"equal\")\n",
    "    plt.xlim(0, 2499)\n",
    "    plt.ylim(1999, 0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "_ = interact(\n",
    "    plot_with_sliders,\n",
    "    rx=FloatSlider(min=-np.pi, max=np.pi, step=0.01, value=0.0),\n",
    "    ry=FloatSlider(min=-np.pi, max=np.pi, step=0.01, value=0.0),\n",
    "    rz=FloatSlider(min=-np.pi, max=np.pi, step=0.01, value=0.0),\n",
    "    tx=FloatSlider(min=-10.0, max=10.0, step=1.0, value=0.0),\n",
    "    ty=FloatSlider(min=-10.0, max=10.0, step=1.0, value=0.0),\n",
    "    tz=FloatSlider(min=50, max=200.0, step=1.0, value=100.0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective-n-Point (PnP) pose computation\n",
    "\n",
    "A limitation in having 1 2D camera is that, even though we can project a 3D point to a 2D point, we cannot go from a 2D point to a 3D point. This is because any point along the line from the camera center through the 2D point will project to the same 2D point. If we on the other hand have a know 3D object, we can utilize the constraints that the 3D object gives us to estimate the pose of the camera. Estimating the pose of the camera is called Perspective-n-Point (PnP) pose computation and what we try to do is to find the rotation vector and the translation vector that would give the desired projection. In this case we have detected an object in the 2D image and we know how the corresponding 3D coordinates of the object correspond to each other. We will create this using the grid we created above, mainly because it gives us a simple way to create a dataset. And the case above is very similar to capturing a planar calibration object, like a checkerboard. That said, this would work with any know 3D object, a cool example could be that you take an image of New York City and you know the relative positions of the buildings, then you could estimate the where the image was taken from.\n",
    "\n",
    "The PnP problem is a non-linear optimization problem, and there are several algorithms to solve it. The most common one is the [Levenberg-Marquardt](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm) algorithm, which is a combination of the Gauss-Newton algorithm and the steepest descent algorithm. The Levenberg-Marquardt algorithm is implemented in OpenCV with the `cv2.solvePnP` function. Here we will simply use [Gauss-Newton](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm) to solve the PnP problem. Let's first create a dataset, that is we will use the above projection and grid with a small adjustment to the rotation vector and translation vector. We will then end up with:\n",
    "\n",
    "- 3D points: The grid we created above, located in the $xy$-plane with $z=0$.\n",
    "- 2D points: The corresponding 2D points of the 3D points.\n",
    "\n",
    "Let's create and plot the dataset:\n",
    "\n",
    ":::{note}\n",
    "It is worth mentioning that the PnP problem is a non-linear optimization problem, and it is sensitive to the initial guess. This is why we need to provide a good initial guess to the PnP algorithm. For this course we will limit us self to fairly small rotations and translations from the initial guess. In a real application requires a more robust initialization and possibly a more robust optimization algorithm (like the Levenberg-Marquardt algorithm).\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "rvec_to_find = np.array([0.1, 0.2, 0.3], dtype=np.float32)\n",
    "tvec_to_find = np.array([5.6, -4.5, 98.7], dtype=np.float32)\n",
    "\n",
    "detected_pixels = project_points(\n",
    "    points=points.reshape(-1, 3),\n",
    "    rvec=rvec_to_find,\n",
    "    tvec=tvec_to_find,\n",
    "    lens_model=lens_model,\n",
    ").reshape(10, 10, 2)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(detected_pixels[..., 0], detected_pixels[..., 1], \"x-\")\n",
    "plt.plot(detected_pixels[..., 0].T, detected_pixels[..., 1].T, \"-\")\n",
    "plt.axis(\"equal\")\n",
    "plt.xlim(0, 2499)\n",
    "plt.ylim(1999, 0)\n",
    "plt.title(\n",
    "    f\"rvec: [{rvec_to_find[0]:.1f}, {rvec_to_find[1]:.1f}, {rvec_to_find[2]:.1f}], tvec: [{tvec_to_find[0]:.1f}, {tvec_to_find[1]:.1f}, {tvec_to_find[2]:.1f}]\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV\n",
    "\n",
    "Just to show that this works we can use the OpenCV function `cv2.solvePnP` to solve the PnP problem and see what the resulting $rvec$ and $tvec$ are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, opencv_rvec, opencv_tvec = cv2.solvePnP(\n",
    "    objectPoints=points.reshape(-1, 3),\n",
    "    imagePoints=detected_pixels.reshape(-1, 2),\n",
    "    cameraMatrix=lens_model.camera_matrix.as_matrix(),\n",
    "    distCoeffs=lens_model.distortion_coefficients.as_opencv_vector(),\n",
    ")\n",
    "opencv_rvec = opencv_rvec.squeeze()\n",
    "opencv_tvec = opencv_tvec.squeeze()\n",
    "\n",
    "print(\n",
    "    f\"rvec: [{rvec_to_find[0]:.1f}, {rvec_to_find[1]:.1f}, {rvec_to_find[2]:.1f}], opencv_rvec: [{opencv_rvec[0]:.3f}, {opencv_rvec[1]:.3f}, {opencv_rvec[2]:.3f}]\"\n",
    ")\n",
    "print(\n",
    "    f\"tvec: [{tvec_to_find[0]:.1f}, {tvec_to_find[1]:.1f}, {tvec_to_find[2]:.1f}], opencv_tvec: [{opencv_tvec[0]:.3f}, {opencv_tvec[1]:.3f}, {opencv_tvec[2]:.3f}]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our implementation\n",
    "\n",
    "It is not a suprise that OpenCV works, but can we implement this ourself? Let's try to implement the Gauss-Newton algorithm to solve the PnP problem. The Gauss-Newton algorithm is an iterative algorithm that tries to minimize the error between the observed 2D points and the projected 3D points. The algorithm is iterative and the update step is given by:\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{cc} r_{\\text{new}} \\\\ t_{\\text{new}} \\end{array}\\right] = \\left[\\begin{array}{cc} r \\\\ t \\end{array}\\right] - \\left(J^T J\\right)^{-1} J^T E\n",
    "$$\n",
    "\n",
    "where $r$ and $t$ are the current estimates of the rotation vector and translation vector, $J$ is the Jacobian matrix of the error function and $E$ is the error vector. The error vector is the difference between the observed 2D points and the projected 3D points. The Jacobian matrix is the derivative of the error vector with respect to the rotation vector and translation vector. We could get the Jacobian matrix our self but we will use the `cv2.projectPoints` function for this. The `cv2.projectPoints` function will give us the Jacobian matrix as well as the projected 2D points. \n",
    "\n",
    "Here is an implementation we can use to get both the projection and the jacobian matrix:\n",
    "\n",
    ":::{note}\n",
    "You might see that I only extract the first 6 values of the jacobian matrix, these values correspond to the rotation vector and translation vector. The rest of the values of the jacobian matrix corresponds to the camera matrix and the distortion coefficients. We will not use these values since we only are solving the PnP problem, but this is actually how you do a full camera calibration for all the parameters in the camera model. Although that require a lot more 2D-3D correspondences to reach the required rank.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_points_with_jacobian(\n",
    "    points: NDArray[Shape[\"*, 3\"], Float32],\n",
    "    rvec: NDArray[Shape[\"3\"], Float32],\n",
    "    tvec: NDArray[Shape[\"3\"], Float32],\n",
    "    lens_model: LensModel,\n",
    ") -> tuple[NDArray[Shape[\"*, 2\"], Float32], NDArray[Shape[\"*, 2, 6\"], Float32]]:\n",
    "    projected_points, jacobian = cv2.projectPoints(\n",
    "        objectPoints=points,\n",
    "        rvec=rvec,\n",
    "        tvec=tvec,\n",
    "        cameraMatrix=lens_model.camera_matrix.as_matrix(),\n",
    "        distCoeffs=lens_model.distortion_coefficients.as_opencv_vector(),\n",
    "    )\n",
    "    return (\n",
    "        projected_points.astype(np.float32)[:, 0, :],\n",
    "        jacobian.astype(np.float32)[:, :6].reshape(-1, 2, 6),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have everything we need to implement the Gauss-Newton algorithm to solve the PnP problem. The step-by-step approach could look somehing like:\n",
    "\n",
    "1. Initialize the rotation vector and translation vector to some initial guess, its important that we don't start with a values that make the $Z$-coordinates 0. We can for example start with $r = [0, 0, 0]^T$ and $t = [0, 0, 100]^T$.\n",
    "2. Project the 3D points to the 2D points using the current estimates of the rotation vector and translation vector, getting both the projected 2D points and the Jacobian matrix.\n",
    "3. Compute the error vector as the difference between the observed 2D points and the projected 2D points. $E = P_{\\text{obs}} - P_{\\text{proj}}$.\n",
    "4. Reshape the error vector to a column vector, from shape (N, 2) to shape (2N,).\n",
    "5. Rehsape the Jacobian matrix to a matrix with shape (2N, 6).\n",
    "6. Compute the update step using the Gauss-Newton algorithm: $\\Delta = [\\Delta r_x, \\Delta r_y, \\Delta r_z, \\Delta t_x, \\Delta t_y, \\Delta t_z]^T = (J^T J)^{-1} J^T E$. Hint: Use `np.linalg.lstsq` to solve the linear system.\n",
    "7. Apply the update step to the current estimates of the rotation vector and translation vector: $r_{\\text{new}} = r - \\Delta r$ and $t_{\\text{new}} = t - \\Delta t$.\n",
    "8. Repeat steps 2-7 until the error is below a certain threshold or a maximum number of iterations is reached. The update error can be defined as the norm of the update step: $\\text{error} = \\|\\Delta\\|$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_pnp(\n",
    "    points: NDArray[Shape[\"*, 3\"], Float32],\n",
    "    pixels: NDArray[Shape[\"*, 2\"], Float32],\n",
    "    lens_model: LensModel,\n",
    "    rvec: NDArray[Shape[\"3\"], Float32] = np.zeros(3, dtype=np.float32),\n",
    "    tvec: NDArray[Shape[\"3\"], Float32] = np.zeros(3, dtype=np.float32),\n",
    "    epsilon: float = 1e-5,\n",
    "    max_iterations: int = 100,\n",
    ") -> tuple[NDArray[Shape[\"3\"], Float32], NDArray[Shape[\"3\"], Float32]]: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvec_initial = np.array([0.0, 0.0, 0.0], dtype=np.float32)\n",
    "tvec_initial = np.array([0.0, 0.0, 100.0], dtype=np.float32)\n",
    "\n",
    "rvec_tvec = solve_pnp(\n",
    "    points=points.reshape(-1, 3),\n",
    "    pixels=detected_pixels.reshape(-1, 2),\n",
    "    lens_model=lens_model,\n",
    "    rvec=rvec_initial,\n",
    "    tvec=tvec_initial,\n",
    ")\n",
    "rvec_found, tvec_found = (\n",
    "    (\n",
    "        rvec_initial * np.nan,\n",
    "        tvec_initial * np.nan,\n",
    "    )\n",
    "    if rvec_tvec is None\n",
    "    else (rvec_tvec[0], rvec_tvec[1])\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"rvec: [{rvec_to_find[0]:.1f}, {rvec_to_find[1]:.1f}, {rvec_to_find[2]:.1f}], rvec_found: [{rvec_found[0]:.3f}, {rvec_found[1]:.3f}, {rvec_found[2]:.3f}]\"\n",
    ")\n",
    "print(\n",
    "    f\"tvec: [{tvec_to_find[0]:.1f}, {tvec_to_find[1]:.1f}, {tvec_to_find[2]:.1f}], tvec_found: [{tvec_found[0]:.3f}, {tvec_found[1]:.3f}, {tvec_found[2]:.3f}]\"\n",
    ")\n",
    "\n",
    "all_close = np.allclose(rvec_found, rvec_to_find, atol=1e-3) and np.allclose(\n",
    "    tvec_found, tvec_to_find, atol=1e-3\n",
    ")\n",
    "print(f\"The solution is {'correct' if all_close else 'incorrect'}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
